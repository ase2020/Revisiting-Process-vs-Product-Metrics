{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,roc_auc_score,recall_score,precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from src import SMOTE\n",
    "from src import CFS\n",
    "from src import metrices_V2 as metrices\n",
    "\n",
    "import platform\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(df):\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    cols = df.columns\n",
    "    smt = SMOTE.smote(df)\n",
    "    df = smt.run()\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def apply_cfs(df):\n",
    "        y = df.Bugs.values\n",
    "        X = df.drop(labels = ['Bugs'],axis = 1)\n",
    "        X = X.values\n",
    "        selected_cols = CFS.cfs(X,y)\n",
    "        cols = df.columns[[selected_cols]].tolist()\n",
    "        cols.append('Bugs')\n",
    "        return df[cols],cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_commit_level(project,metric):\n",
    "    understand_path = 'data/understand_files_all/' + project + '_understand.csv'\n",
    "    understand_df = pd.read_csv(understand_path)\n",
    "    understand_df = understand_df.dropna(axis = 1,how='all')\n",
    "    cols_list = understand_df.columns.values.tolist()\n",
    "    for item in ['Kind', 'Name','commit_hash', 'Bugs']:\n",
    "        if item in cols_list:\n",
    "            cols_list.remove(item)\n",
    "            cols_list.insert(0,item)\n",
    "    understand_df = understand_df[cols_list]\n",
    "    cols = understand_df.columns.tolist()\n",
    "    understand_df = understand_df.drop_duplicates(cols[4:len(cols)])\n",
    "    understand_df['Name'] = understand_df.Name.str.rsplit('.',1).str[1]\n",
    "    \n",
    "    commit_guru_file_level_path = 'data/commit_guru_file/' + project + '.csv'\n",
    "    commit_guru_file_level_df = pd.read_csv(commit_guru_file_level_path)\n",
    "    commit_guru_file_level_df['commit_hash'] = commit_guru_file_level_df.commit_hash.str.strip('\"')\n",
    "    commit_guru_file_level_df = commit_guru_file_level_df[commit_guru_file_level_df['file_name'].str.contains('.java')]\n",
    "    commit_guru_file_level_df['Name'] = commit_guru_file_level_df.file_name.str.rsplit('/',1).str[1].str.split('.').str[0].str.replace('/','.')\n",
    "    commit_guru_file_level_df = commit_guru_file_level_df.drop('file_name',axis = 1)\n",
    "    \n",
    "    \n",
    "    df = understand_df.merge(commit_guru_file_level_df,how='left',on=['commit_hash','Name'])\n",
    "    \n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove('Bugs')\n",
    "    cols.append('Bugs')\n",
    "    df = df[cols]\n",
    "    commit_hash = df.commit_hash\n",
    "    Name = df.Name\n",
    "    for item in ['Kind', 'Name','commit_hash']:\n",
    "        if item in cols:\n",
    "            df = df.drop(labels = [item],axis=1)\n",
    "#     df.dropna(inplace=True)\n",
    "    df = df.drop_duplicates()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    y = df.Bugs\n",
    "    X = df.drop('Bugs',axis = 1)\n",
    "    cols = X.columns\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    imp_mean = IterativeImputer(random_state=0)\n",
    "    X = imp_mean.fit_transform(X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    \n",
    "    if metric == 'process':\n",
    "        X = X[['file_la', 'file_ld', 'file_lt', 'file_age', 'file_ddev',\n",
    "       'file_nuc', 'own', 'minor', 'file_ndev', 'file_ncomm', 'file_adev',\n",
    "       'file_nadev', 'file_avg_nddev', 'file_avg_nadev', 'file_avg_ncomm',\n",
    "       'file_ns', 'file_exp', 'file_sexp', 'file_rexp', 'file_nd', 'file_sctr']]\n",
    "    elif metric == 'product':\n",
    "        X = X.drop(['file_la', 'file_ld', 'file_lt', 'file_age', 'file_ddev',\n",
    "       'file_nuc', 'own', 'minor', 'file_ndev', 'file_ncomm', 'file_adev',\n",
    "       'file_nadev', 'file_avg_nddev', 'file_avg_nadev', 'file_avg_ncomm',\n",
    "       'file_ns', 'file_exp', 'file_sexp', 'file_rexp', 'file_nd', 'file_sctr'],axis = 1)\n",
    "    else:\n",
    "        X = X\n",
    "    \n",
    "    X['Name'] = Name\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=18)\n",
    "    \n",
    "    test_df = X_test\n",
    "    test_df['Bugs'] = y_test\n",
    "    \n",
    "    train_df = X_train\n",
    "    train_df['Bugs'] = y_train\n",
    "    \n",
    "    \n",
    "    defective_train_files = set(train_df[train_df['Bugs'] == 1].Name.values.tolist())\n",
    "    non_defective_train_files = set(train_df[train_df['Bugs'] == 0].Name.values.tolist())\n",
    "    \n",
    "    defective_non_defective_train_files = defective_train_files.intersection(non_defective_train_files)\n",
    "    \n",
    "    only_defective_train_files = defective_train_files - defective_non_defective_train_files\n",
    "    \n",
    "    only_non_defective_train_files = non_defective_train_files - defective_non_defective_train_files\n",
    "    \n",
    "    test_df_recurruing = test_df[test_df['Bugs'] == 1]\n",
    "    test_df_recurruing = test_df_recurruing[test_df_recurruing.Name.isin(defective_train_files)]\n",
    "    \n",
    "    test_df_test_only = test_df[test_df['Bugs'] == 1]\n",
    "    test_df_test_only = test_df_test_only[test_df_test_only.Name.isin(only_non_defective_train_files)]\n",
    "    \n",
    "    test_df_train_only = test_df[test_df['Bugs'] == 0]\n",
    "    test_df_train_only = test_df_train_only[test_df_train_only.Name.isin(only_defective_train_files)]\n",
    "    \n",
    "    y_train = train_df.Bugs\n",
    "    X_train = train_df.drop(['Bugs','Name'],axis = 1)\n",
    "    \n",
    "    \n",
    "    test_non_defective = test_df[test_df['Bugs'] == 0]\n",
    "    test_defective = test_df[test_df['Bugs'] == 1]\n",
    "    \n",
    "    \n",
    "    test_df_recurruing = test_df_recurruing.drop(['Name'],axis = 1)\n",
    "    test_df_test_only = test_df_test_only.drop(['Name'],axis = 1)\n",
    "    test_df_train_only = test_df_train_only.drop(['Name'],axis = 1)\n",
    "    \n",
    "\n",
    "    return X_train,y_train,test_df_recurruing, test_df_train_only, test_df_test_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self(project,metric):\n",
    "    \n",
    "    pf = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f1 = []\n",
    "    g_score = []\n",
    "    pci_20 = []\n",
    "    ifa = []\n",
    "    auc = []\n",
    "    \n",
    "    X_train,y_train,test_df_recurruing, test_df_train_only, test_df_test_only = load_data_commit_level(project,metric)\n",
    "    \n",
    "    df_smote = pd.concat([X_train,y_train],axis = 1)\n",
    "#     print(df_smote)\n",
    "    df_smote = apply_smote(df_smote)\n",
    "    y_train = df_smote.Bugs\n",
    "    X_train = df_smote.drop('Bugs',axis = 1)\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train,y_train)\n",
    "    importance = clf.feature_importances_\n",
    "    \n",
    "    # recurrence only\n",
    "    try:\n",
    "        y_test = test_df_recurruing.Bugs\n",
    "        X_test = test_df_recurruing.drop('Bugs',axis=1)\n",
    "        if metric == 'process':\n",
    "            loc = X_test['file_la'] + X_test['file_lt']\n",
    "        elif metric == 'product':\n",
    "            loc = X_test.CountLineCode\n",
    "        else:\n",
    "            loc = X_test['file_la'] + X_test['file_lt']\n",
    "\n",
    "        predicted = clf.predict(X_test)\n",
    "        abcd = metrices.measures(y_test,predicted,loc)\n",
    "\n",
    "        pf.append(abcd.get_pf())\n",
    "        recall.append(abcd.calculate_recall())\n",
    "        precision.append(abcd.calculate_precision())\n",
    "        f1.append(abcd.calculate_f1_score())\n",
    "        g_score.append(abcd.get_g_score())\n",
    "        pci_20.append(abcd.get_pci_20())\n",
    "        ifa.append(abcd.get_ifa())\n",
    "        try:\n",
    "            auc.append(roc_auc_score(y_test, predicted))\n",
    "        except:\n",
    "            auc.append(0)\n",
    "        print(classification_report(y_test, predicted))\n",
    "    except:\n",
    "        print(test_df_recurruing.shape)\n",
    "        pf.append(-1)\n",
    "        recall.append(-1)\n",
    "        precision.append(1)\n",
    "        f1.append(-1)\n",
    "        g_score.append(-1)\n",
    "        pci_20.append(-1)\n",
    "        ifa.append(-1)\n",
    "        auc.append(-1)\n",
    "    \n",
    "    # train only\n",
    "    try:\n",
    "        y_test = test_df_train_only.Bugs\n",
    "        X_test = test_df_train_only.drop('Bugs',axis=1)\n",
    "        if metric == 'process':\n",
    "            loc = X_test['file_la'] + X_test['file_lt']\n",
    "        elif metric == 'product':\n",
    "            loc = X_test.CountLineCode\n",
    "        else:\n",
    "            loc = X_test['file_la'] + X_test['file_lt']\n",
    "\n",
    "        predicted = clf.predict(X_test)\n",
    "        abcd = metrices.measures(y_test,predicted,loc)\n",
    "\n",
    "        pf.append(abcd.get_pf())\n",
    "        recall.append(abcd.calculate_recall())\n",
    "        precision.append(abcd.calculate_precision())\n",
    "        f1.append(abcd.calculate_f1_score())\n",
    "        g_score.append(abcd.get_g_score())\n",
    "        pci_20.append(abcd.get_pci_20())\n",
    "        ifa.append(abcd.get_ifa())\n",
    "        try:\n",
    "            auc.append(roc_auc_score(y_test, predicted))\n",
    "        except:\n",
    "            auc.append(0)\n",
    "        print(classification_report(y_test, predicted))\n",
    "    except:\n",
    "        print(test_df_train_only.shape)\n",
    "        pf.append(-1)\n",
    "        recall.append(-1)\n",
    "        precision.append(1)\n",
    "        f1.append(-1)\n",
    "        g_score.append(-1)\n",
    "        pci_20.append(-1)\n",
    "        ifa.append(-1)\n",
    "        auc.append(-1)\n",
    "    \n",
    "    # test only\n",
    "    try:\n",
    "        y_test = test_df_test_only.Bugs\n",
    "        X_test = test_df_test_only.drop('Bugs',axis=1)\n",
    "        if metric == 'process':\n",
    "            loc = X_test['file_la'] + X_test['file_lt']\n",
    "        elif metric == 'product':\n",
    "            loc = X_test.CountLineCode\n",
    "        else:\n",
    "            loc = X_test['file_la'] + X_test['file_lt']\n",
    "\n",
    "        predicted = clf.predict(X_test)\n",
    "        abcd = metrices.measures(y_test,predicted,loc)\n",
    "\n",
    "        pf.append(abcd.get_pf())\n",
    "        recall.append(abcd.calculate_recall())\n",
    "        precision.append(abcd.calculate_precision())\n",
    "        f1.append(abcd.calculate_f1_score())\n",
    "        g_score.append(abcd.get_g_score())\n",
    "        pci_20.append(abcd.get_pci_20())\n",
    "        ifa.append(abcd.get_ifa())\n",
    "        try:\n",
    "            auc.append(roc_auc_score(y_test, predicted))\n",
    "        except:\n",
    "            auc.append(0)\n",
    "        print(classification_report(y_test, predicted))\n",
    "    except:\n",
    "        print(test_df_test_only.shape)\n",
    "        pf.append(-1)\n",
    "        recall.append(-1)\n",
    "        precision.append(1)\n",
    "        f1.append(-1)\n",
    "        g_score.append(-1)\n",
    "        pci_20.append(-1)\n",
    "        ifa.append(-1)\n",
    "        auc.append(-1)\n",
    "    \n",
    "    return recall,precision,pf,f1,g_score,auc,pci_20,ifa,importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_df = pd.read_csv('projects.csv')\n",
    "projects = proj_df.repo_name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "types = ['process','product']\n",
    "for _type in types:\n",
    "    precision_list = {}\n",
    "    recall_list = {}\n",
    "    pf_list = {}\n",
    "    f1_list = {}\n",
    "    g_list = {}\n",
    "    auc_list = {}\n",
    "    pci_20_list = {}\n",
    "    ifa_list = {}\n",
    "    featue_importance = {}\n",
    "    for project in projects:\n",
    "        try:\n",
    "            if project == '.DS_Store':\n",
    "                continue\n",
    "            print(\"+++++++++++++++++   \"  + project + \"  +++++++++++++++++\")\n",
    "            recall,precision,pf,f1,g_score,auc,pci_20,ifa,importance = run_self(project,_type)\n",
    "            recall_list[project] = recall\n",
    "            precision_list[project] = precision\n",
    "            pf_list[project] = pf\n",
    "            f1_list[project] = f1\n",
    "            g_list[project] = g_score\n",
    "            auc_list[project] = auc\n",
    "            pci_20_list[project] = pci_20\n",
    "            ifa_list[project] = ifa\n",
    "            featue_importance[project] = importance\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    final_result = {}\n",
    "    final_result['precision'] = precision_list\n",
    "    final_result['recall'] = recall_list\n",
    "    final_result['pf'] = pf_list\n",
    "    final_result['f1'] = f1_list\n",
    "    final_result['g'] = g_list\n",
    "    final_result['auc'] = auc_list\n",
    "    final_result['pci_20'] = pci_20_list\n",
    "    final_result['ifa'] = ifa_list\n",
    "    final_result['featue_importance'] = featue_importance\n",
    "    with open('results/Final_results/RQ6_' + _type + '.pkl', 'wb') as handle:\n",
    "        pickle.dump(final_result, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "\n",
    "order = ['recurrent','train only','test only']\n",
    "\n",
    "for metric_type in ['process','product']:\n",
    "    file_df = pd.DataFrame()\n",
    "    with open('results/Final_results/RQ6_' + metric_type + '.pkl', 'rb') as handle:\n",
    "        final_result = pickle.load(handle)\n",
    "        for goal in final_result.keys():\n",
    "            score_r = []\n",
    "            score_train = []\n",
    "            score_test = []\n",
    "            sub_df = pd.DataFrame()\n",
    "            for project in final_result[goal].keys():\n",
    "\n",
    "                if np.isnan(final_result[goal][project][0]):\n",
    "                    score_r.append(0)\n",
    "                elif final_result[goal][project][0] != -1:\n",
    "                    score_r.append(np.nanmedian(final_result[goal][project][0]))\n",
    "\n",
    "                if np.isnan(final_result[goal][project][1]):\n",
    "                    score_train.append(0)\n",
    "                elif final_result[goal][project][1] != -1:\n",
    "                    score_train.append(np.nanmedian(final_result[goal][project][1]))\n",
    "\n",
    "                if np.isnan(final_result[goal][project][2]):\n",
    "                    score_test.append(0)    \n",
    "                elif final_result[goal][project][2] != -1:\n",
    "                    score_test.append(np.nanmedian(final_result[goal][project][2]))\n",
    "\n",
    "            all_scores = score_r + score_train + score_test\n",
    "            all_order = [order[0]]*len(score_r) + [order[1]]*len(score_train) + [order[2]]*len(score_test)\n",
    "            df = pd.DataFrame(zip(all_scores,all_order), columns = ['score','test_type'])\n",
    "\n",
    "            sub_df = pd.concat([sub_df,df], axis = 0)\n",
    "            sub_df['metric'] = [goal]*sub_df.shape[0]\n",
    "            file_df = pd.concat([file_df,sub_df])\n",
    "        file_df['metric type'] = [metric_type]*file_df.shape[0]\n",
    "        final_df = pd.concat([final_df,file_df], axis = 0)\n",
    "    \n",
    "\n",
    "final_df = final_df[final_df.metric.isin(['recall','pf'])]            \n",
    "#     print(\"Goal:\",goal,np.median(score_r),np.median(score_train),np.median(score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.replace({'recall':'Recall','pf':'Pf','process':'P','product':'C'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid',font_scale=1.4)\n",
    "order = [\"P\", \"C\"]\n",
    "g = sns.catplot(x=\"metric type\", y=\"score\", col=\"test_type\",row=\"metric\",height=4,aspect=0.6,margin_titles=True,kind=\"box\", \n",
    "                order=order, data=final_df)\n",
    "[plt.setp(ax.texts, text=\"\") for ax in g.axes.flat]\n",
    "g.set_titles(row_template = '{row_name}', col_template = '{col_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
